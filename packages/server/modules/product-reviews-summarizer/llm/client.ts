import { InferenceClient } from '@huggingface/inference'
import { Ollama } from 'ollama'
import OpenAI from 'openai'
import summarizePrompt from '../llm/prompts/summarize-reviews.txt'

const openAIClient = new OpenAI({
   apiKey: process.env.OPENAI_API_KEY,
})

const inferenceClient = new InferenceClient(process.env.HF_TOKEN)

const ollamaClient = new Ollama()

type GenerateTextOptions = {
   model?: string
   prompt: string
   instructions?: string
   temperature?: number
   maxTokens?: number
   previousResponseId?: string
}

type GenerateTextResult = {
   id: string
   text: string
}

export const llmClient = {
   async generateText({
      model = 'gpt-4.1',
      prompt,
      instructions,
      temperature = 0.2,
      maxTokens = 300,
      previousResponseId,
   }: GenerateTextOptions): Promise<GenerateTextResult> {
      const response = await openAIClient.responses.create({
         model,
         input: prompt,
         instructions,
         temperature,
         max_output_tokens: maxTokens,
         previous_response_id: previousResponseId,
      })

      return {
         id: response.id,
         text: response.output_text,
      }
   },

   async summarizeReviews(reviews: string) {
      const prompt = `
      Summarize the following customer reviews into a short paragraph
highlighting key themes, both positive and negative:

         ${reviews}
      `
      const response = await openAIClient.responses.create({
         model: 'gpt-4.1',
         input: prompt,
         temperature: 0.2,
         max_output_tokens: 500,
      })

      return response.output_text
   },
}
